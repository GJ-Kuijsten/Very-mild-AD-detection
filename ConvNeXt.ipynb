{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f2180509-436c-4679-bd94-33254d983968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torch in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: nibabel in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (5.3.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\gjkku\\appdata\\roaming\\python\\python312\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: timm in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (1.0.15)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from nibabel) (24.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from timm) (0.29.3)\n",
      "Requirement already satisfied: safetensors in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: requests in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (4.66.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas torch torchvision nibabel opencv-python timm scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "03681216-d3f5-4434-9da2-cdaad54db800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNeXt\n",
    "\n",
    "\n",
    "# loading all libraries used\n",
    "import pandas as pd # for handling tabular data\n",
    "import os # for interacting with file system\n",
    "import torch # PyTorch library\n",
    "from torch.utils.data import Dataset, DataLoader # for dataset and batching\n",
    "from torchvision import transforms # for applying image transformation (data augmentation)\n",
    "from PIL import Image # for image loading and applying the transforms\n",
    "import numpy as np # for numerical operations\n",
    "import nibabel as nib # For loading medical imaging files (.hdr and .img files)\n",
    "from sklearn.model_selection import train_test_split # for splitting the data into train and test set\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc # metrics used for evaluation of the model\n",
    "import matplotlib.pyplot as plt # for plotting and visualisations\n",
    "import cv2 # for image processing\n",
    "import timm # For loading the pretrained model\n",
    "import torch.nn as nn # layers of the Neural Network\n",
    "import torch.optim as optim # using the AdamW optimizer and the scheduler\n",
    "import time # To measure training time\n",
    "import csv # to write metrics into a csv file\n",
    "from torchvision.transforms.functional import gaussian_blur # additional transform to add blurring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "92df13b6-6e1c-490a-8086-175d6b9384be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'M/F', 'Hand', 'Age', 'Educ', 'SES', 'MMSE', 'CDR', 'eTIV',\n",
      "       'nWBV', 'ASF', 'Delay', 'Class', 'MRI_Path'],\n",
      "      dtype='object')\n",
      "   CDR  label\n",
      "0  0.0      0\n",
      "1  0.0      0\n",
      "2  0.5      1\n",
      "8  0.0      0\n",
      "9  0.0      0\n",
      "label\n",
      "0    135\n",
      "1     70\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# loading csv file with all paths to the MRI files and subject information\n",
    "df = pd.read_csv(r\"C:\\Users\\gjkku\\OneDrive\\Documenten\\CSAI year 3\\Thesis\\csv_binary\\binary_with_mri_paths.csv\", sep='\\t')\n",
    "\n",
    "# columns of the csv file to understand the structure of the dataset\n",
    "print(df.columns)\n",
    "\n",
    "# using only subjects that are non-demented (CDR 0) and very mild demented (CDR 0.5) \n",
    "df = df[df['CDR'].isin([0.0, 0.5])].dropna(subset=['CDR'])\n",
    "\n",
    "# converting to binary labels: CDR 0 = 0 and CDR 0.5 = 1\n",
    "df['label'] = df['CDR'].apply(lambda x: 0 if x == 0.0 else 1)\n",
    "\n",
    "# printing the first 5 subjects with CDR 0 or CDR 0.5 and the count of each class to check\n",
    "print(df[['CDR', 'label']].head())\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5b2fd9fd-8a68-44b2-bcfe-5c668f400b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    135\n",
      "1    135\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# dataset is imbalanced -> oversampling the minority class (CDR 0.5 / label 1) to match the majority class (CDR 0/ label 0)\n",
    "df_class_0 = df[df['label'] == 0]\n",
    "df_class_1 = df[df['label'] == 1].sample(n=135, replace=True, random_state=42)\n",
    "\n",
    "# combining both classes and shuffle the dataset\n",
    "balanced_df = pd.concat([df_class_0, df_class_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#printing class counts to check if dataset is balanced\n",
    "print(balanced_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9364e5af-bafa-4bb5-8503-61e0557639ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set class distribution:\n",
      "label\n",
      "0    108\n",
      "1    108\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set class distribution:\n",
      "label\n",
      "0    27\n",
      "1    27\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# dataset split into 80% train/20% validation\n",
    "train_set, validation_set = train_test_split(balanced_df, test_size=0.2, stratify=balanced_df['label'], random_state=42) # stratify -> equal numbers of each class\n",
    "\n",
    "# distribution of the train dataset\n",
    "print(\"Train set class distribution:\")\n",
    "print(train_set['label'].value_counts())\n",
    "\n",
    "# distribution of the validation dataset\n",
    "print(\"\\nValidation set class distribution:\")\n",
    "print(validation_set['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "58bc1a16-6e4f-43fc-9c4a-90485f18236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load and preprocess MRI files (.img/.hdr files)\n",
    "def MRI_slicing(img_path, target_size=(224, 224), axis_slice=1):\n",
    "    # loading .img/.hdr file (NiBabel finds the img and then automatically finds the .hdr file that belongs to it)\n",
    "    img = nib.load(img_path)\n",
    "    data = img.get_fdata()\n",
    "    \n",
    "    # normalizing the image using z-score normalization -> formula = (X - mean)/ std.dev\n",
    "    data = (data - np.mean(data)) / np.std(data)\n",
    "    \n",
    "    # getting the coronal slice of the 3D MRI image (axis_slice: sagittal = 0, coronal = 1, axial = 2)\n",
    "    middle_slice = data.shape[axis_slice] // 2\n",
    "    if axis_slice == 0: \n",
    "        image_slice = data[middle_slice, :, :]\n",
    "    elif axis_slice == 1:\n",
    "        image_slice = data[:, middle_slice, :]\n",
    "    else:\n",
    "        image_slice = data[:, :, middle_slice]\n",
    "    \n",
    "    # resizing image to (224, 224) using OpenCV\n",
    "    image_slice = cv2.resize(image_slice, target_size, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # normalizing pixel values between the range [0, 1]\n",
    "    image_slice = (image_slice - np.min(image_slice)) / (np.max(image_slice) - np.min(image_slice))\n",
    "\n",
    "    # returns the 2D image slice as a float32 array\n",
    "    return image_slice.astype(np.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4cb3cd6f-321e-4ce2-8592-7fde1523a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing a PyTorch dataset for loading images and labels\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, axis_slice=1):\n",
    "        # initialize with a dataframe, transforms, and the axis for slicing\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.axis_slice = axis_slice\n",
    "    \n",
    "    def __len__(self):\n",
    "        # returns the number of samples in the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # getting the i-th sample: image path + label\n",
    "        row = self.data.iloc[i]\n",
    "        img_path = row['MRI_Path']  \n",
    "        label = int(row['label'])\n",
    "\n",
    "        # using the slicing function to load and preprocess the image\n",
    "        image_slice = MRI_slicing(img_path, axis_slice=self.axis_slice)\n",
    "\n",
    "        # convert the grayscale images to RGB, else the pretrained models can't use it\n",
    "        image_slice = np.stack([image_slice]*3, axis=-1)  # (H, W, 3)\n",
    "\n",
    "        # applying transforms/data augmentation (transforms are getting defined in a later cell)\n",
    "        if self.transform:\n",
    "            image_slice = self.transform(Image.fromarray((image_slice * 255).astype(np.uint8)))\n",
    "            \n",
    "        # returning an image tensor with the assigned label\n",
    "        return image_slice, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8fd7b169-726f-465f-9daa-37a307874f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "\n",
    "# transform for training using flipping, rotating, affine, color jitter, normalizing\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomAffine(degrees=5, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "# transform that uses noise for training (This is used after epoch 10 as defined in the training loop)\n",
    "transform_with_noise = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomAffine(degrees=5, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "    # data augmentation for testing robustness\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5)), # gaussian blur\n",
    "    transforms.Lambda(lambda x: x + 0.01 * torch.randn_like(x)) # random noise\n",
    "])\n",
    "\n",
    "# validation transform using only normalizing (no noise for validation set)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7526ebf2-6e50-40e6-8053-1347f7c80d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the PyTorch dataset for training and validation\n",
    "train_dataset = MRIDataset(train_set, transform=transform)\n",
    "val_dataset = MRIDataset(validation_set, transform=val_transform) # -> no noise for the validation set\n",
    "\n",
    "# using the PyTorch dataloader to create batches and shuffling the batch\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f75bef67-456d-4119-9279-f2e2ea42e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# loading pretrained ConvNeXt tiny model for binary classification with the timm library\n",
    "model = timm.create_model('convnext_tiny', pretrained=True, num_classes=2)\n",
    "\n",
    "# freeze early layers (stage 0) to prevent overfitting\n",
    "for name, param in model.named_parameters():\n",
    "    if 'stages.0' in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# modifying classifier head with dropout for regularization\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5), # 50% dropout regulation -> randomly selected neurons are ignored during training -> helps generalizing\n",
    "    nn.Linear(model.head.in_features, 2) # linear layer for 2 classes\n",
    ")\n",
    "\n",
    "# moving model to specified device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e9955c54-ceb3-49a7-b3c1-8d1d452f8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss -> cross-entropy loss with label smoothing for binary classification\n",
    "loss_function = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# optimizer -> AdamW with learning rate and weight decay\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# learning rate scheduler: OneCycleLR\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-4, total_steps=len(train_loader)*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3d80a14f-df2e-4767-a4ea-8ff1cdd8476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train and evaluate the model\n",
    "def train(model, train_loader, validation_loader, loss_function, optimizer, scheduler, device, epochs=50):\n",
    "    \n",
    "    # keeping track of the best model to save it\n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        # using noisy training data after epoch 10 \n",
    "        if epoch == 10:\n",
    "            print(\"Noisy training data is now being used\")\n",
    "            train_dataset.transform = transform_with_noise\n",
    "            \n",
    "        # training mode keeping track of total loss over each epoch and calculating accuracy for each epoch\n",
    "        model.train()\n",
    "        loss_over_epoch = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        # iterate over training batches\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # forward pass, computing loss, backpropagation to compute gradients and updating the weights of the model\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            \n",
    "            # tracking loss and accuracy\n",
    "            loss_over_epoch += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        # computing train accuracy \n",
    "        train_accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss_over_epoch:.4f} - Train accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        \n",
    "        # evaluation mode\n",
    "        model.eval()\n",
    "        val_correct, val_total = 0, 0\n",
    "\n",
    "        # lists to store all predictions, true labels, and probabilities for computing metrics of the validation set\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        probabilities = []\n",
    "\n",
    "        # iterate over validation batches (without gradient computation)\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                # raw logits of the model \n",
    "                outputs = model(images)\n",
    "\n",
    "                # computing probabilities for class 1 (CDR 0.5) using softmax and predictions using argmax\n",
    "                probs = torch.softmax(outputs, dim=1)[:, 1] # -> probability of class 1 (CDR 0.5)\n",
    "                preds = torch.argmax(outputs, dim=1) \n",
    "\n",
    "                # storing the predictions, true labels, and probabilities into the lists\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "                probabilities.extend(probs.cpu().numpy())\n",
    "                \n",
    "                \n",
    "        # metrics (accuracy, precision, recall, F1-score, AUC)\n",
    "        validation_accuracy = 100 * (np.array(predictions) == np.array(true_labels)).sum() / len(true_labels)\n",
    "        precision = precision_score(true_labels, predictions)\n",
    "        recall = recall_score(true_labels, predictions)\n",
    "        f1 = f1_score(true_labels, predictions)\n",
    "        auc = roc_auc_score(true_labels, probabilities)\n",
    "        \n",
    "        # saving the best model if it improved the validation accuracy\n",
    "        if validation_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = validation_accuracy\n",
    "            torch.save(model.state_dict(), 'ConvNeXt_best_model.pth')\n",
    "            print(\"The best model is saved\")\n",
    "\n",
    "        # printing metrics of each epoch\n",
    "        print(f\"Validation Accuracy: {validation_accuracy:.2f}%\")\n",
    "        print(f\"Precision: {precision:.2f} | Recall: {recall:.2f} | F1: {f1:.2f} | AUC: {auc:.2f}\\n\")\n",
    "\n",
    "       \n",
    "        \n",
    "##########################################################################################################################\n",
    "\n",
    "    # load the best model to re-evaluate and save metrics of the best model\n",
    "    best_model_path = r\"C:\\Users\\gjkku\\ConvNeXt_best_model.pth\"\n",
    "    if os.path.exists(best_model_path):\n",
    "        point = torch.load(best_model_path, map_location=device)\n",
    "        model.load_state_dict(point)\n",
    "        model.eval()\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"file not found\")\n",
    "\n",
    "    # lists to store all predictions, true labels, and probabilities for computing metrics of the validation set\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    probabilities = []\n",
    "\n",
    "    # iterate over validation batches (without gradient computation)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # raw logits of the model \n",
    "            outputs = model(images)\n",
    "\n",
    "            # computing probabilities for class 1 (CDR 0.5) using softmax and predictions using argmax\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1] # -> probability of class 1 (CDR 0.5)\n",
    "            preds = torch.argmax(outputs, dim=1) \n",
    "\n",
    "            # storing the predictions, true labels, and probabilities into the lists\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            probabilities.extend(probs.cpu().numpy())\n",
    "                \n",
    "                \n",
    "    # metrics (accuracy, precision, recall, F1-score, AUC)\n",
    "    validation_accuracy = 100 * (np.array(predictions) == np.array(true_labels)).sum() / len(true_labels)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    auc = roc_auc_score(true_labels, probabilities)\n",
    "    \n",
    "    # SQ1: saving the detection accuracy and other metrics into a CSV file\n",
    "    sq1 = {\n",
    "        \"model\": \"ConvNeXt_tiny\",\n",
    "        \"validation accuracy\": round(validation_accuracy * 100, 2),\n",
    "        \"precision\": round(precision, 2),\n",
    "        \"recall\": round(recall, 2),\n",
    "        \"f1\": round(f1, 2),\n",
    "        \"auc\": round(auc, 2)\n",
    "    }\n",
    "\n",
    "    with open(\"SQ1_ConvNeXt.csv\", \"a\", newline='', encoding='utf-8') as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(sq1.keys()))\n",
    "        if f.tell() == 0:\n",
    "           w.writeheader()\n",
    "        w.writerow(sq1)\n",
    "\n",
    "    # returning true labels and probabilities\n",
    "    return true_labels, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fd7ea9f7-26d6-43fe-a582-95cdf9eaeb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Loss: 4.8599 - Train accuracy: 62.04%\n",
      "The best model is saved\n",
      "Validation Accuracy: 51.85%\n",
      "Precision: 0.51 | Recall: 0.81 | F1: 0.63 | AUC: 0.74\n",
      "\n",
      "Epoch 2/50 - Loss: 4.4872 - Train accuracy: 63.89%\n",
      "The best model is saved\n",
      "Validation Accuracy: 66.67%\n",
      "Precision: 0.80 | Recall: 0.44 | F1: 0.57 | AUC: 0.80\n",
      "\n",
      "Epoch 3/50 - Loss: 4.0328 - Train accuracy: 70.83%\n",
      "Validation Accuracy: 51.85%\n",
      "Precision: 0.51 | Recall: 1.00 | F1: 0.68 | AUC: 0.86\n",
      "\n",
      "Epoch 4/50 - Loss: 4.5324 - Train accuracy: 66.20%\n",
      "The best model is saved\n",
      "Validation Accuracy: 81.48%\n",
      "Precision: 0.84 | Recall: 0.78 | F1: 0.81 | AUC: 0.89\n",
      "\n",
      "Epoch 5/50 - Loss: 3.6885 - Train accuracy: 78.24%\n",
      "Validation Accuracy: 79.63%\n",
      "Precision: 0.90 | Recall: 0.67 | F1: 0.77 | AUC: 0.88\n",
      "\n",
      "Epoch 6/50 - Loss: 3.3890 - Train accuracy: 79.63%\n",
      "Validation Accuracy: 81.48%\n",
      "Precision: 0.77 | Recall: 0.89 | F1: 0.83 | AUC: 0.91\n",
      "\n",
      "Epoch 7/50 - Loss: 3.3656 - Train accuracy: 81.02%\n",
      "Validation Accuracy: 81.48%\n",
      "Precision: 0.90 | Recall: 0.70 | F1: 0.79 | AUC: 0.92\n",
      "\n",
      "Epoch 8/50 - Loss: 3.2851 - Train accuracy: 82.41%\n",
      "Validation Accuracy: 79.63%\n",
      "Precision: 0.90 | Recall: 0.67 | F1: 0.77 | AUC: 0.90\n",
      "\n",
      "Epoch 9/50 - Loss: 2.8231 - Train accuracy: 87.96%\n",
      "The best model is saved\n",
      "Validation Accuracy: 85.19%\n",
      "Precision: 0.81 | Recall: 0.93 | F1: 0.86 | AUC: 0.90\n",
      "\n",
      "Epoch 10/50 - Loss: 3.3631 - Train accuracy: 78.70%\n",
      "Validation Accuracy: 55.56%\n",
      "Precision: 1.00 | Recall: 0.11 | F1: 0.20 | AUC: 0.91\n",
      "\n",
      "Noisy training data is now being used\n",
      "Epoch 11/50 - Loss: 3.5824 - Train accuracy: 73.61%\n",
      "Validation Accuracy: 83.33%\n",
      "Precision: 0.91 | Recall: 0.74 | F1: 0.82 | AUC: 0.93\n",
      "\n",
      "Epoch 12/50 - Loss: 2.9608 - Train accuracy: 86.57%\n",
      "Validation Accuracy: 83.33%\n",
      "Precision: 0.88 | Recall: 0.78 | F1: 0.82 | AUC: 0.91\n",
      "\n",
      "Epoch 13/50 - Loss: 3.2738 - Train accuracy: 85.65%\n",
      "Validation Accuracy: 77.78%\n",
      "Precision: 1.00 | Recall: 0.56 | F1: 0.71 | AUC: 0.94\n",
      "\n",
      "Epoch 14/50 - Loss: 3.2023 - Train accuracy: 81.94%\n",
      "Validation Accuracy: 83.33%\n",
      "Precision: 0.76 | Recall: 0.96 | F1: 0.85 | AUC: 0.96\n",
      "\n",
      "Epoch 15/50 - Loss: 2.3175 - Train accuracy: 93.06%\n",
      "Validation Accuracy: 57.41%\n",
      "Precision: 1.00 | Recall: 0.15 | F1: 0.26 | AUC: 0.94\n",
      "\n",
      "Epoch 16/50 - Loss: 2.5677 - Train accuracy: 87.96%\n",
      "The best model is saved\n",
      "Validation Accuracy: 87.04%\n",
      "Precision: 0.86 | Recall: 0.89 | F1: 0.87 | AUC: 0.96\n",
      "\n",
      "Epoch 17/50 - Loss: 1.9004 - Train accuracy: 96.30%\n",
      "Validation Accuracy: 87.04%\n",
      "Precision: 0.83 | Recall: 0.93 | F1: 0.88 | AUC: 0.97\n",
      "\n",
      "Epoch 18/50 - Loss: 1.7781 - Train accuracy: 98.15%\n",
      "The best model is saved\n",
      "Validation Accuracy: 90.74%\n",
      "Precision: 0.89 | Recall: 0.93 | F1: 0.91 | AUC: 0.95\n",
      "\n",
      "Epoch 19/50 - Loss: 1.6055 - Train accuracy: 100.00%\n",
      "The best model is saved\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.95\n",
      "\n",
      "Epoch 20/50 - Loss: 1.5952 - Train accuracy: 99.54%\n",
      "Validation Accuracy: 88.89%\n",
      "Precision: 0.96 | Recall: 0.81 | F1: 0.88 | AUC: 0.94\n",
      "\n",
      "Epoch 21/50 - Loss: 1.7157 - Train accuracy: 98.61%\n",
      "The best model is saved\n",
      "Validation Accuracy: 94.44%\n",
      "Precision: 0.96 | Recall: 0.93 | F1: 0.94 | AUC: 0.95\n",
      "\n",
      "Epoch 22/50 - Loss: 1.5473 - Train accuracy: 99.54%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.94\n",
      "\n",
      "Epoch 23/50 - Loss: 1.5892 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.95\n",
      "\n",
      "Epoch 24/50 - Loss: 1.4817 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.94\n",
      "\n",
      "Epoch 25/50 - Loss: 1.4782 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.96\n",
      "\n",
      "Epoch 26/50 - Loss: 1.4517 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.98\n",
      "\n",
      "Epoch 27/50 - Loss: 1.4445 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.96\n",
      "\n",
      "Epoch 28/50 - Loss: 1.4509 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n",
      "Epoch 29/50 - Loss: 1.4795 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.95\n",
      "\n",
      "Epoch 30/50 - Loss: 1.4829 - Train accuracy: 99.07%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n",
      "Epoch 31/50 - Loss: 1.4419 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n",
      "Epoch 32/50 - Loss: 1.4635 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n",
      "Epoch 33/50 - Loss: 1.4245 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.96\n",
      "\n",
      "Epoch 34/50 - Loss: 1.4271 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n",
      "Epoch 35/50 - Loss: 1.4163 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 90.74%\n",
      "Precision: 0.89 | Recall: 0.93 | F1: 0.91 | AUC: 0.97\n",
      "\n",
      "Epoch 36/50 - Loss: 1.4168 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n",
      "Epoch 37/50 - Loss: 1.4221 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.95\n",
      "\n",
      "Epoch 38/50 - Loss: 1.4274 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.95\n",
      "\n",
      "Epoch 39/50 - Loss: 1.4310 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 90.74%\n",
      "Precision: 0.89 | Recall: 0.93 | F1: 0.91 | AUC: 0.97\n",
      "\n",
      "Epoch 40/50 - Loss: 1.4195 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 90.74%\n",
      "Precision: 0.89 | Recall: 0.93 | F1: 0.91 | AUC: 0.97\n",
      "\n",
      "Epoch 41/50 - Loss: 1.4206 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n",
      "Epoch 42/50 - Loss: 1.4117 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n",
      "Epoch 43/50 - Loss: 1.4160 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n",
      "Epoch 44/50 - Loss: 1.4060 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n",
      "Epoch 45/50 - Loss: 1.4096 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n",
      "Epoch 46/50 - Loss: 1.4112 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n",
      "Epoch 47/50 - Loss: 1.4112 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n",
      "Epoch 48/50 - Loss: 1.4149 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n",
      "Epoch 49/50 - Loss: 1.4053 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n",
      "Epoch 50/50 - Loss: 1.4055 - Train accuracy: 100.00%\n",
      "Validation Accuracy: 92.59%\n",
      "Precision: 0.93 | Recall: 0.93 | F1: 0.93 | AUC: 0.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# keeping track of time (start-time)\n",
    "start_time = time.time()\n",
    "\n",
    "# training model for 50 epochs\n",
    "true_labels, probabilities = train(model, train_loader, validation_loader, loss_function, optimizer, scheduler, device, epochs=50)\n",
    "\n",
    "# keeping track of time (end-time)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3b1906fb-3ae8-4ba4-ac77-960cc9eefcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQ2: calculating inference time, memory usage, and overall resource consumption and saving it into a CSV file\n",
    "\n",
    "# load the best model\n",
    "best_model_path = r\"C:\\Users\\gjkku\\ConvNeXt_best_model.pth\"\n",
    "if os.path.exists(best_model_path):\n",
    "    point = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(point)\n",
    "    model.eval()\n",
    "else:\n",
    "    raise FileNotFoundError(f\"file not found\")        \n",
    "\n",
    "# computing average inference time\n",
    "sample = next(iter(validation_loader))[0][0].unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    inf_start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = model(sample)\n",
    "    inf_end = time.time()\n",
    "\n",
    "sq2 = {\n",
    "    \"model\": \"ConvNeXt_tiny\",\n",
    "    \"training_time_sec\": round(end_time - start_time, 2), \n",
    "    \"model_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "    \"model_size_MB\": round(os.path.getsize('ConvNeXt_best_model.pth') / (1024 * 1024), 2),\n",
    "    \"avg_inference_time_s\": round((inf_end - inf_start)/100, 4)\n",
    "}\n",
    "\n",
    "# saving the metrics for SQ2 into a CSV file\n",
    "with open(\"SQ2_ConvNeXt.csv\", \"a\", newline='') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=sq2.keys())\n",
    "    if f.tell() == 0:\n",
    "       w.writeheader()\n",
    "    w.writerow(sq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eb107ea2-5c13-46a0-9ab5-487a352dc382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQ3: evaluating robustness by adding noise and blur to validation data\n",
    "\n",
    "best_model_path = r\"C:\\Users\\gjkku\\ConvNeXt_best_model.pth\"\n",
    "if os.path.exists(best_model_path):\n",
    "    point = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(point)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"file not found\")\n",
    "    \n",
    "def evaluate_with_noise(model, loader, noise_std=0.2, blur=False):\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # lists to store all predictions, true labels, and probabilities for computing metrics of the validation set\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # adding noise\n",
    "            if noise_std > 0:\n",
    "                images = torch.clamp(images + torch.randn_like(images) * noise_std, 0., 1.)\n",
    "            # adding blur\n",
    "            if blur:\n",
    "                images = torch.stack([gaussian_blur(img, kernel_size=3) for img in images])\n",
    "            \n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "            # storing the predictions, true labels, and probabilities into the lists\n",
    "            probabilities.extend(probs.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # returning the metrics\n",
    "    return {\n",
    "        \"acc\": accuracy_score(true_labels, predictions),\n",
    "        \"precision\": precision_score(true_labels, predictions, average='macro', zero_division=0),\n",
    "        \"recall\": recall_score(true_labels, predictions, average='macro', zero_division=0),\n",
    "        \"f1\": f1_score(true_labels, predictions, average='macro', zero_division=0),\n",
    "        \"auc\": roc_auc_score(true_labels, probabilities) if len(set(true_labels)) > 1 else 0.0\n",
    "    }\n",
    "\n",
    "# Run robustness tests\n",
    "test_types = [\n",
    "    # only noise\n",
    "    {\"type\": \"noise\", \"args\": {\"noise_std\": 0.2, \"blur\": False}},\n",
    "    # only blur\n",
    "    {\"type\": \"blur\", \"args\": {\"noise_std\": 0.0, \"blur\": True}},\n",
    "    # both\n",
    "    {\"type\": \"noise+blur\", \"args\": {\"noise_std\": 0.2, \"blur\": True}}\n",
    "]\n",
    "\n",
    "# save robustness test metrics into a csf file\n",
    "with open(\"SQ3_ConvNeXt.csv\", \"a\", newline='') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"model\", \"type\", \"acc\", \"precision\", \"recall\", \"f1\", \"auc\"])\n",
    "    if f.tell() == 0:\n",
    "        w.writeheader()\n",
    "    for test in test_types:\n",
    "        result = evaluate_with_noise(model, validation_loader, **test[\"args\"])\n",
    "        w.writerow({\"model\": \"ConvNeXt_tiny\", \"type\": test[\"type\"], **result})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "896ec095-52ba-4599-8ac2-0a9ece265818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model_path = r\"C:\\Users\\gjkku\\ConvNeXt_best_model.pth\"\n",
    "if os.path.exists(best_model_path):\n",
    "    point = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(point)\n",
    "    model.eval()\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Best model file not found at {best_model_path}\")\n",
    "\n",
    "# lists to store all true labels and predictions\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in validation_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# computation of the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-Demented', 'Very Mild Demented'])\n",
    "\n",
    "# confusion matrix plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for ConvNeXt')\n",
    "plt.savefig('ConvNeXt_confusion_matrix.png')  \n",
    "plt.show()  \n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
