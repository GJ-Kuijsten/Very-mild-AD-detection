{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a6550a5-b0ad-45c2-a09d-973d87c37b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torch in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: nibabel in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (5.3.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\gjkku\\appdata\\roaming\\python\\python312\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: timm in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (1.0.15)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from nibabel) (24.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from timm) (0.31.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: requests in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (4.66.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gjkku\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas torch torchvision nibabel opencv-python timm scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10cca86b-3da9-42a4-a1ce-cabc7880cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNet-V2\n",
    "\n",
    "# loading all libraries used\n",
    "import pandas as pd # for handling tabular data\n",
    "import os # for interacting with file system\n",
    "import torch # PyTorch library\n",
    "from torch.utils.data import Dataset, DataLoader # for dataset and batching\n",
    "from torchvision import transforms # for applying image transformation (data augmentation)\n",
    "from PIL import Image # for image loading and applying the transforms\n",
    "import numpy as np # for numerical operations\n",
    "import nibabel as nib # For loading medical imaging files (.hdr and .img files)\n",
    "from sklearn.model_selection import train_test_split # for splitting the data into train and test set\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc # metrics used for evaluation of the model\n",
    "import matplotlib.pyplot as plt # for plotting and visualisations\n",
    "import cv2 # for image processing\n",
    "import timm # For loading the pretrained model\n",
    "import torch.nn as nn # layers of the Neural Network\n",
    "import torch.optim as optim # using the AdamW optimizer and the scheduler\n",
    "import time # To measure training time\n",
    "import csv # to write metrics into a csv file\n",
    "from torchvision.transforms.functional import gaussian_blur # additional transform to add blurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81439a4e-5dd8-4e69-8f7f-569c2defec48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'M/F', 'Hand', 'Age', 'Educ', 'SES', 'MMSE', 'CDR', 'eTIV',\n",
      "       'nWBV', 'ASF', 'Delay', 'Class', 'MRI_Path'],\n",
      "      dtype='object')\n",
      "   CDR  label\n",
      "0  0.0      0\n",
      "1  0.0      0\n",
      "2  0.5      1\n",
      "8  0.0      0\n",
      "9  0.0      0\n",
      "label\n",
      "0    135\n",
      "1     70\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# loading csv file with all paths to the MRI files and subject information\n",
    "df = pd.read_csv(r\"C:\\Users\\gjkku\\OneDrive\\Documenten\\CSAI year 3\\Thesis\\csv_binary\\binary_with_mri_paths.csv\", sep='\\t')\n",
    "\n",
    "# columns of the csv file to understand the structure of the dataset\n",
    "print(df.columns)\n",
    "\n",
    "# using only subjects that are non-demented (CDR 0) and very mild demented (CDR 0.5) \n",
    "df = df[df['CDR'].isin([0.0, 0.5])].dropna(subset=['CDR'])\n",
    "\n",
    "# converting to binary labels: CDR 0 = 0 and CDR 0.5 = 1\n",
    "df['label'] = df['CDR'].apply(lambda x: 0 if x == 0.0 else 1)\n",
    "\n",
    "# printing the first 5 subjects with CDR 0 or CDR 0.5 and the count of each class to check\n",
    "print(df[['CDR', 'label']].head())\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e958995c-3c96-4454-847b-04784b425319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    135\n",
      "1    135\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# dataset is imbalanced -> oversampling the minority class (CDR 0.5 / label 1) to match the majority class (CDR 0/ label 0)\n",
    "df_class_0 = df[df['label'] == 0]\n",
    "df_class_1 = df[df['label'] == 1].sample(n=135, replace=True, random_state=42)\n",
    "\n",
    "# merging the balanced dataset\n",
    "balanced_df = pd.concat([df_class_0, df_class_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(balanced_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5337c090-ae69-422c-a753-808d6debbf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set class distribution:\n",
      "label\n",
      "0    108\n",
      "1    108\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set class distribution:\n",
      "label\n",
      "0    27\n",
      "1    27\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# dataset split into 80% train/20% validation\n",
    "train_set, validation_set = train_test_split(balanced_df, test_size=0.2, stratify=balanced_df['label'], random_state=42) # stratify -> equal numbers of each class\n",
    "\n",
    "# distribution of the train dataset\n",
    "print(\"Train set class distribution:\")\n",
    "print(train_set['label'].value_counts())\n",
    "\n",
    "# distribution of the validation dataset\n",
    "print(\"\\nValidation set class distribution:\")\n",
    "print(validation_set['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f580e639-706c-4112-a878-81a30c9ecefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load and preprocess MRI files (.img/.hdr files)\n",
    "def MRI_slicing(img_path, target_size=(224, 224), axis_slice=1):\n",
    "    # loading .img/.hdr file (NiBabel finds the img and then automatically finds the .hdr file that belongs to it)\n",
    "    img = nib.load(img_path)\n",
    "    data = img.get_fdata()\n",
    "    \n",
    "    # normalizing the image using z-score normalization -> formula = (X - mean)/ std.dev\n",
    "    data = (data - np.mean(data)) / np.std(data)\n",
    "    \n",
    "    # getting the coronal slice of the 3D MRI image (axis_slice: sagittal = 0, coronal = 1, axial = 2)\n",
    "    middle_slice = data.shape[axis_slice] // 2\n",
    "    if axis_slice == 0: \n",
    "        image_slice = data[middle_slice, :, :]\n",
    "    elif axis_slice == 1:\n",
    "        image_slice = data[:, middle_slice, :]\n",
    "    else:\n",
    "        image_slice = data[:, :, middle_slice]\n",
    "    \n",
    "    # resizing image to (224, 224) using OpenCV\n",
    "    image_slice = cv2.resize(image_slice, target_size, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # normalizing pixel values between the range [0, 1]\n",
    "    image_slice = (image_slice - np.min(image_slice)) / (np.max(image_slice) - np.min(image_slice))\n",
    "\n",
    "    # returns the 2D image slice as a float32 array\n",
    "    return image_slice.astype(np.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d26d31-15af-4679-a55d-a2438560365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing a PyTorch dataset for loading images and labels\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, axis_slice=1):\n",
    "        # initialize with a dataframe, transforms, and the axis for slicing\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.axis_slice = axis_slice\n",
    "    \n",
    "    def __len__(self):\n",
    "        # returns the number of samples in the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # getting the i-th sample: image path + label\n",
    "        row = self.data.iloc[i]\n",
    "        img_path = row['MRI_Path']  \n",
    "        label = int(row['label'])\n",
    "\n",
    "        # using the slicing function to load and preprocess the image\n",
    "        image_slice = MRI_slicing(img_path, axis_slice=self.axis_slice)\n",
    "\n",
    "        # convert the grayscale images to RGB, else the pretrained models can't use it\n",
    "        image_slice = np.stack([image_slice]*3, axis=-1)  # (H, W, 3)\n",
    "\n",
    "        # applying transforms/data augmentation (transforms are getting defined in a later cell)\n",
    "        if self.transform:\n",
    "            image_slice = self.transform(Image.fromarray((image_slice * 255).astype(np.uint8)))\n",
    "            \n",
    "        # returning an image tensor with the assigned label\n",
    "        return image_slice, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34ee775e-a3a3-48bb-ae46-dd029b529c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "\n",
    "# transform for training using flipping, rotating, affine, color jitter, normalizing\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomAffine(degrees=5, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "# transform that uses noise for training (This is used after epoch 10 as defined in the training loop)\n",
    "transform_with_noise = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomAffine(degrees=5, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "    # data augmentation for testing robustness\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5)), # gaussian blur\n",
    "    transforms.Lambda(lambda x: x + 0.01 * torch.randn_like(x)) # random noise\n",
    "])\n",
    "\n",
    "# validation transform using only normalizing (no noise for validation set)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "523f7cd5-ffdf-486f-ac3f-b2eae1f8ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the PyTorch dataset for training and validation\n",
    "train_dataset = MRIDataset(train_set, transform=transform)\n",
    "val_dataset = MRIDataset(validation_set, transform=val_transform) # -> no noise for the validation set\n",
    "\n",
    "# using the PyTorch dataloader to create batches and shuffling the batch\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12dabd1b-74be-4cc6-90a6-4a1e05b9aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading pretrained MobileNetV2 for binary classification with Timm library\n",
    "model = timm.create_model('mobilenetv2_100', pretrained=True, num_classes=2) # dropout regulation -> randomly selected neurons are ignored during training -> helps generalizing\n",
    "\n",
    "# modifying classifier head with dropout for regularization\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5), # dropout regularization -> randomly selected neurons are ignored during training -> helps generalizing\n",
    "    nn.Linear(model.classifier.in_features, 2)\n",
    ")\n",
    "\n",
    "# moving model to specified device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceeae356-e24c-4c81-8e1c-1210e19e4609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss -> cross-entropy loss with label smoothing for binary classification\n",
    "loss_function = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# optimizer -> AdamW with learning rate\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86204d10-ba54-4114-8ad9-8aad3a8788fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, validation_loader, loss_function, optimizer, device, epochs=40):\n",
    "    # keeping track of the best model to save it\n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        # using noisy training data after epoch 10 (without this the model did not perform well earlier)\n",
    "        if epoch == 10:\n",
    "            print(\"Epoch 10: noisy training data is now being used\")\n",
    "            train_dataset.transform = transform_with_noise\n",
    "        # training mode keeping track of total loss over each epoch and calculating accuracy for each epoch\n",
    "        model.train()\n",
    "        loss_over_epoch = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        #getting a batch of images and labels to train on\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # forward pass, model predicting, computing loss, backpropagation to compute gradients and updating the weights of the model\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # tracking loss and accuracy of the epoch\n",
    "            loss_over_epoch += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        # computing train accuracy\n",
    "        train_accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss_over_epoch:.4f} - Train accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        \n",
    "        # evaluation mode\n",
    "        model.eval()\n",
    "        val_correct, val_total = 0, 0\n",
    "\n",
    "        # lists to store all predictions, true labels, and probabilities for computing metrics of the validation set\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        probabilities = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                # raw logits of the model \n",
    "                outputs = model(images)\n",
    "                \n",
    "                # computing probabilities for class 1 (CDR 0.5) using softmax and predictions using argmax\n",
    "                probs = torch.softmax(outputs, dim=1)[:, 1] # -> probability of class 1 (CDR 0.5)\n",
    "                # picking the predicted class using argmax\n",
    "                preds = torch.argmax(outputs, dim=1) \n",
    "\n",
    "                # storing the predictions, true labels, and probabilities into the lists\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "                probabilities.extend(probs.cpu().numpy())\n",
    "                \n",
    "                \n",
    "        # metrics (accuracy, precision, recall, F1-score, AUC)\n",
    "        validation_accuracy = 100 * (np.array(predictions) == np.array(true_labels)).sum() / len(true_labels)\n",
    "        precision = precision_score(true_labels, predictions)\n",
    "        recall = recall_score(true_labels, predictions)\n",
    "        f1 = f1_score(true_labels, predictions)\n",
    "        auc = roc_auc_score(true_labels, probabilities)\n",
    "\n",
    "        # saving the best model if it improved the validation accuracy\n",
    "        if validation_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = validation_accuracy\n",
    "            torch.save(model.state_dict(), 'MobileNet-V2_best_model.pth')\n",
    "            print(\"The best model is saved\")\n",
    "\n",
    "        # printing metrics of each epoch\n",
    "        print(f\"Validation Accuracy: {validation_accuracy:.2f}%\")\n",
    "        print(f\"Precision: {precision:.2f} | Recall: {recall:.2f} | F1: {f1:.2f} | AUC: {auc:.2f}\\n\")\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "    # load the best model to re-evaluate and save metrics of the best model\n",
    "    best_model_path = r\"C:\\Users\\gjkku\\MobileNet-V2_best_model.pth\"\n",
    "    if os.path.exists(best_model_path):\n",
    "        point = torch.load(best_model_path, map_location=device)\n",
    "        model.load_state_dict(point)\n",
    "        model.eval()\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"file not found\") \n",
    "\n",
    "    # lists to store all predictions, true labels, and probabilities for computing metrics of the validation set\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    probabilities = []\n",
    "\n",
    "    # iterate over validation batches (without gradient computation)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # raw logits of the model \n",
    "            outputs = model(images)\n",
    "                \n",
    "            # computing probabilities for class 1 (CDR 0.5) using softmax and predictions using argmax\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1] # -> probability of class 1 (CDR 0.5)\n",
    "            # picking the predicted class using argmax\n",
    "            preds = torch.argmax(outputs, dim=1) \n",
    "\n",
    "            # storing the predictions, true labels, and probabilities into the lists\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            probabilities.extend(probs.cpu().numpy())\n",
    "                \n",
    "                \n",
    "    # metrics (accuracy, precision, recall, F1-score, AUC)\n",
    "    validation_accuracy = 100 * (np.array(predictions) == np.array(true_labels)).sum() / len(true_labels)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    auc = roc_auc_score(true_labels, probabilities)\n",
    "\n",
    "    # SQ1: saving the detection accuracy and other metrics into a CSV file\n",
    "    sq1 = {\n",
    "        \"model\": \"MobileNet-V2\",\n",
    "        \"validation accuracy\": round(validation_accuracy * 100, 2),\n",
    "        \"precision\": round(precision, 2),\n",
    "        \"recall\": round(recall, 2),\n",
    "        \"f1\": round(f1, 2),\n",
    "        \"auc\": round(auc, 2)\n",
    "    }\n",
    "\n",
    "    with open(\"SQ1_MobileNet-V2.csv\", \"a\", newline='', encoding='utf-8') as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(sq1.keys()))\n",
    "        if f.tell() == 0:\n",
    "           w.writeheader()\n",
    "        w.writerow(sq1)\n",
    "\n",
    "    # returning true labels and probabilities\n",
    "    return true_labels, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e84bb5c-4024-469e-911b-7535bb803f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Loss: 5.0355 - Train accuracy: 50.46%\n",
      "The best model is saved\n",
      "Validation Accuracy: 48.15%\n",
      "Precision: 0.49 | Recall: 0.96 | F1: 0.65 | AUC: 0.44\n",
      "\n",
      "Epoch 2/50 - Loss: 4.8810 - Train accuracy: 52.78%\n",
      "The best model is saved\n",
      "Validation Accuracy: 50.00%\n",
      "Precision: 0.50 | Recall: 1.00 | F1: 0.67 | AUC: 0.51\n",
      "\n",
      "Epoch 3/50 - Loss: 4.6752 - Train accuracy: 60.19%\n",
      "Validation Accuracy: 50.00%\n",
      "Precision: 0.50 | Recall: 1.00 | F1: 0.67 | AUC: 0.60\n",
      "\n",
      "Epoch 4/50 - Loss: 4.6845 - Train accuracy: 61.57%\n",
      "The best model is saved\n",
      "Validation Accuracy: 51.85%\n",
      "Precision: 0.51 | Recall: 1.00 | F1: 0.68 | AUC: 0.68\n",
      "\n",
      "Epoch 5/50 - Loss: 4.4676 - Train accuracy: 66.67%\n",
      "The best model is saved\n",
      "Validation Accuracy: 62.96%\n",
      "Precision: 0.57 | Recall: 1.00 | F1: 0.73 | AUC: 0.74\n",
      "\n",
      "Epoch 6/50 - Loss: 4.5054 - Train accuracy: 63.43%\n",
      "Validation Accuracy: 62.96%\n",
      "Precision: 0.57 | Recall: 1.00 | F1: 0.73 | AUC: 0.79\n",
      "\n",
      "Epoch 7/50 - Loss: 4.3340 - Train accuracy: 67.13%\n",
      "Validation Accuracy: 59.26%\n",
      "Precision: 0.55 | Recall: 0.96 | F1: 0.70 | AUC: 0.76\n",
      "\n",
      "Epoch 8/50 - Loss: 4.2798 - Train accuracy: 68.52%\n",
      "The best model is saved\n",
      "Validation Accuracy: 64.81%\n",
      "Precision: 0.60 | Recall: 0.93 | F1: 0.72 | AUC: 0.77\n",
      "\n",
      "Epoch 9/50 - Loss: 4.2686 - Train accuracy: 68.06%\n",
      "Validation Accuracy: 64.81%\n",
      "Precision: 0.62 | Recall: 0.74 | F1: 0.68 | AUC: 0.76\n",
      "\n",
      "Epoch 10/50 - Loss: 3.9496 - Train accuracy: 74.07%\n",
      "The best model is saved\n",
      "Validation Accuracy: 74.07%\n",
      "Precision: 0.81 | Recall: 0.63 | F1: 0.71 | AUC: 0.77\n",
      "\n",
      "Epoch 10: noisy training data is now being used\n",
      "Epoch 11/50 - Loss: 4.1236 - Train accuracy: 69.91%\n",
      "Validation Accuracy: 74.07%\n",
      "Precision: 0.69 | Recall: 0.89 | F1: 0.77 | AUC: 0.88\n",
      "\n",
      "Epoch 12/50 - Loss: 4.0775 - Train accuracy: 73.61%\n",
      "The best model is saved\n",
      "Validation Accuracy: 81.48%\n",
      "Precision: 0.81 | Recall: 0.81 | F1: 0.81 | AUC: 0.88\n",
      "\n",
      "Epoch 13/50 - Loss: 4.0890 - Train accuracy: 71.76%\n",
      "Validation Accuracy: 72.22%\n",
      "Precision: 0.83 | Recall: 0.56 | F1: 0.67 | AUC: 0.86\n",
      "\n",
      "Epoch 14/50 - Loss: 4.2076 - Train accuracy: 72.22%\n",
      "Validation Accuracy: 70.37%\n",
      "Precision: 0.79 | Recall: 0.56 | F1: 0.65 | AUC: 0.87\n",
      "\n",
      "Epoch 15/50 - Loss: 3.7993 - Train accuracy: 75.00%\n",
      "Validation Accuracy: 72.22%\n",
      "Precision: 0.88 | Recall: 0.52 | F1: 0.65 | AUC: 0.87\n",
      "\n",
      "Epoch 16/50 - Loss: 3.6927 - Train accuracy: 75.46%\n",
      "Validation Accuracy: 72.22%\n",
      "Precision: 0.93 | Recall: 0.48 | F1: 0.63 | AUC: 0.87\n",
      "\n",
      "Epoch 17/50 - Loss: 3.6745 - Train accuracy: 75.00%\n",
      "Validation Accuracy: 70.37%\n",
      "Precision: 0.87 | Recall: 0.48 | F1: 0.62 | AUC: 0.87\n",
      "\n",
      "Epoch 18/50 - Loss: 3.5151 - Train accuracy: 79.63%\n",
      "Validation Accuracy: 74.07%\n",
      "Precision: 0.81 | Recall: 0.63 | F1: 0.71 | AUC: 0.86\n",
      "\n",
      "Epoch 19/50 - Loss: 3.6339 - Train accuracy: 78.70%\n",
      "Validation Accuracy: 77.78%\n",
      "Precision: 0.94 | Recall: 0.59 | F1: 0.73 | AUC: 0.87\n",
      "\n",
      "Epoch 20/50 - Loss: 3.6283 - Train accuracy: 76.39%\n",
      "Validation Accuracy: 74.07%\n",
      "Precision: 0.88 | Recall: 0.56 | F1: 0.68 | AUC: 0.87\n",
      "\n",
      "Epoch 21/50 - Loss: 3.4512 - Train accuracy: 79.17%\n",
      "Validation Accuracy: 75.93%\n",
      "Precision: 0.89 | Recall: 0.59 | F1: 0.71 | AUC: 0.87\n",
      "\n",
      "Epoch 22/50 - Loss: 3.3371 - Train accuracy: 81.48%\n",
      "Validation Accuracy: 74.07%\n",
      "Precision: 1.00 | Recall: 0.48 | F1: 0.65 | AUC: 0.88\n",
      "\n",
      "Epoch 23/50 - Loss: 3.2594 - Train accuracy: 84.26%\n",
      "Validation Accuracy: 72.22%\n",
      "Precision: 1.00 | Recall: 0.44 | F1: 0.62 | AUC: 0.88\n",
      "\n",
      "Epoch 24/50 - Loss: 3.1430 - Train accuracy: 82.41%\n",
      "Validation Accuracy: 72.22%\n",
      "Precision: 1.00 | Recall: 0.44 | F1: 0.62 | AUC: 0.90\n",
      "\n",
      "Epoch 25/50 - Loss: 3.3443 - Train accuracy: 81.02%\n",
      "Validation Accuracy: 74.07%\n",
      "Precision: 1.00 | Recall: 0.48 | F1: 0.65 | AUC: 0.90\n",
      "\n",
      "Epoch 26/50 - Loss: 3.2101 - Train accuracy: 83.33%\n",
      "Validation Accuracy: 74.07%\n",
      "Precision: 1.00 | Recall: 0.48 | F1: 0.65 | AUC: 0.90\n",
      "\n",
      "Epoch 27/50 - Loss: 3.0583 - Train accuracy: 85.65%\n",
      "Validation Accuracy: 74.07%\n",
      "Precision: 1.00 | Recall: 0.48 | F1: 0.65 | AUC: 0.90\n",
      "\n",
      "Epoch 28/50 - Loss: 3.0926 - Train accuracy: 84.72%\n",
      "Validation Accuracy: 74.07%\n",
      "Precision: 1.00 | Recall: 0.48 | F1: 0.65 | AUC: 0.90\n",
      "\n",
      "Epoch 29/50 - Loss: 3.1325 - Train accuracy: 82.41%\n",
      "Validation Accuracy: 79.63%\n",
      "Precision: 0.94 | Recall: 0.63 | F1: 0.76 | AUC: 0.91\n",
      "\n",
      "Epoch 30/50 - Loss: 2.9760 - Train accuracy: 87.04%\n",
      "The best model is saved\n",
      "Validation Accuracy: 83.33%\n",
      "Precision: 0.95 | Recall: 0.70 | F1: 0.81 | AUC: 0.91\n",
      "\n",
      "Epoch 31/50 - Loss: 2.9495 - Train accuracy: 83.80%\n",
      "Validation Accuracy: 74.07%\n",
      "Precision: 0.93 | Recall: 0.52 | F1: 0.67 | AUC: 0.92\n",
      "\n",
      "Epoch 32/50 - Loss: 2.8237 - Train accuracy: 87.96%\n",
      "Validation Accuracy: 74.07%\n",
      "Precision: 0.93 | Recall: 0.52 | F1: 0.67 | AUC: 0.92\n",
      "\n",
      "Epoch 33/50 - Loss: 2.8895 - Train accuracy: 85.19%\n",
      "Validation Accuracy: 83.33%\n",
      "Precision: 0.95 | Recall: 0.70 | F1: 0.81 | AUC: 0.93\n",
      "\n",
      "Epoch 34/50 - Loss: 2.9729 - Train accuracy: 86.11%\n",
      "The best model is saved\n",
      "Validation Accuracy: 87.04%\n",
      "Precision: 0.92 | Recall: 0.81 | F1: 0.86 | AUC: 0.92\n",
      "\n",
      "Epoch 35/50 - Loss: 2.8056 - Train accuracy: 88.43%\n",
      "Validation Accuracy: 85.19%\n",
      "Precision: 0.95 | Recall: 0.74 | F1: 0.83 | AUC: 0.93\n",
      "\n",
      "Epoch 36/50 - Loss: 2.7093 - Train accuracy: 86.57%\n",
      "Validation Accuracy: 83.33%\n",
      "Precision: 0.95 | Recall: 0.70 | F1: 0.81 | AUC: 0.93\n",
      "\n",
      "Epoch 37/50 - Loss: 2.5754 - Train accuracy: 89.35%\n",
      "Validation Accuracy: 81.48%\n",
      "Precision: 0.90 | Recall: 0.70 | F1: 0.79 | AUC: 0.94\n",
      "\n",
      "Epoch 38/50 - Loss: 2.5481 - Train accuracy: 92.59%\n",
      "Validation Accuracy: 83.33%\n",
      "Precision: 0.91 | Recall: 0.74 | F1: 0.82 | AUC: 0.94\n",
      "\n",
      "Epoch 39/50 - Loss: 2.4327 - Train accuracy: 92.13%\n",
      "Validation Accuracy: 83.33%\n",
      "Precision: 0.91 | Recall: 0.74 | F1: 0.82 | AUC: 0.94\n",
      "\n",
      "Epoch 40/50 - Loss: 2.4451 - Train accuracy: 91.20%\n",
      "The best model is saved\n",
      "Validation Accuracy: 88.89%\n",
      "Precision: 0.92 | Recall: 0.85 | F1: 0.88 | AUC: 0.94\n",
      "\n",
      "Epoch 41/50 - Loss: 2.3250 - Train accuracy: 92.13%\n",
      "Validation Accuracy: 87.04%\n",
      "Precision: 0.95 | Recall: 0.78 | F1: 0.86 | AUC: 0.94\n",
      "\n",
      "Epoch 42/50 - Loss: 2.3329 - Train accuracy: 95.37%\n",
      "Validation Accuracy: 85.19%\n",
      "Precision: 0.91 | Recall: 0.78 | F1: 0.84 | AUC: 0.95\n",
      "\n",
      "Epoch 43/50 - Loss: 2.2293 - Train accuracy: 93.52%\n",
      "Validation Accuracy: 88.89%\n",
      "Precision: 0.92 | Recall: 0.85 | F1: 0.88 | AUC: 0.94\n",
      "\n",
      "Epoch 44/50 - Loss: 2.2747 - Train accuracy: 92.59%\n",
      "Validation Accuracy: 88.89%\n",
      "Precision: 0.92 | Recall: 0.85 | F1: 0.88 | AUC: 0.94\n",
      "\n",
      "Epoch 45/50 - Loss: 2.2924 - Train accuracy: 92.59%\n",
      "Validation Accuracy: 88.89%\n",
      "Precision: 0.96 | Recall: 0.81 | F1: 0.88 | AUC: 0.95\n",
      "\n",
      "Epoch 46/50 - Loss: 2.4825 - Train accuracy: 88.89%\n",
      "Validation Accuracy: 83.33%\n",
      "Precision: 0.95 | Recall: 0.70 | F1: 0.81 | AUC: 0.95\n",
      "\n",
      "Epoch 47/50 - Loss: 2.2381 - Train accuracy: 94.44%\n",
      "Validation Accuracy: 88.89%\n",
      "Precision: 0.92 | Recall: 0.85 | F1: 0.88 | AUC: 0.95\n",
      "\n",
      "Epoch 48/50 - Loss: 2.0610 - Train accuracy: 93.52%\n",
      "The best model is saved\n",
      "Validation Accuracy: 90.74%\n",
      "Precision: 0.89 | Recall: 0.93 | F1: 0.91 | AUC: 0.95\n",
      "\n",
      "Epoch 49/50 - Loss: 2.1877 - Train accuracy: 93.06%\n",
      "Validation Accuracy: 88.89%\n",
      "Precision: 0.86 | Recall: 0.93 | F1: 0.89 | AUC: 0.95\n",
      "\n",
      "Epoch 50/50 - Loss: 2.1851 - Train accuracy: 94.44%\n",
      "Validation Accuracy: 88.89%\n",
      "Precision: 0.86 | Recall: 0.93 | F1: 0.89 | AUC: 0.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# keeping track of time\n",
    "start_time = time.time()\n",
    "\n",
    "true_labels, probabilities = train(model, train_loader, validation_loader, loss_function, optimizer, device, epochs=50)\n",
    "\n",
    "# keeping track of time\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4672105e-7851-456a-b682-3c422031250b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SQ2: calculating inference time, memory usage, and overall resource consumption and saving it into a CSV file\n",
    "\n",
    "# load the best model\n",
    "best_model_path = r\"C:\\Users\\gjkku\\MobileNet-V2_best_model.pth\"\n",
    "if os.path.exists(best_model_path):\n",
    "    point = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(point)\n",
    "    model.eval()\n",
    "else:\n",
    "    raise FileNotFoundError(f\"file not found\")        \n",
    "\n",
    "# computing average inference time\n",
    "sample = next(iter(validation_loader))[0][0].unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    inf_start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = model(sample)\n",
    "    inf_end = time.time()\n",
    "\n",
    "sq2 = {\n",
    "    \"model\": \"MobileNet-V2\",\n",
    "    \"training_time_sec\": round(end_time - start_time, 2),\n",
    "    \"model_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "    \"mmodel_size_MB\": round(os.path.getsize('MobileNet-V2_best_model.pth') / (1024 * 1024), 2),\n",
    "    \"avg_inference_time_s\": round((inf_end - inf_start)/100, 4)\n",
    "}\n",
    "\n",
    "# saving the metrics for SQ2 into a CSV file\n",
    "with open(\"SQ2_MobileNet-V2.csv\", \"a\", newline='') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=sq2.keys())\n",
    "    if f.tell() == 0:\n",
    "       w.writeheader()\n",
    "    w.writerow(sq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfce243e-966e-4f3b-9363-c09d639b3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQ3: evaluating robustness by adding noise and blur to validation data\n",
    "\n",
    "# load best model\n",
    "best_model_path = r\"C:\\Users\\gjkku\\MobileNet-V2_best_model.pth\"\n",
    "if os.path.exists(best_model_path):\n",
    "    point = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(point)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"file not found\")\n",
    "    \n",
    "def robustness_test(model, loader, noise_std=0.2, blur=False):\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # lists to store all predictions, true labels, and probabilities for computing metrics of the validation set\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # adding noise\n",
    "            if noise_std > 0:\n",
    "                images = torch.clamp(images + torch.randn_like(images) * noise_std, 0., 1.)\n",
    "            # adding blur\n",
    "            if blur:\n",
    "                images = torch.stack([gaussian_blur(img, kernel_size=3) for img in images])\n",
    "            \n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "            # storing the predictions, true labels, and probabilities into the lists\n",
    "            probabilities.extend(probs.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # returning the metrics\n",
    "    return {\n",
    "        \"acc\": accuracy_score(true_labels, predictions),\n",
    "        \"precision\": precision_score(true_labels, predictions, average='macro', zero_division=0),\n",
    "        \"recall\": recall_score(true_labels, predictions, average='macro', zero_division=0),\n",
    "        \"f1\": f1_score(true_labels, predictions, average='macro', zero_division=0),\n",
    "        \"auc\": roc_auc_score(true_labels, probabilities) if len(set(true_labels)) > 1 else 0.0\n",
    "    }\n",
    "\n",
    "# Run robustness tests\n",
    "test_types = [\n",
    "    # only noise\n",
    "    {\"type\": \"noise\", \"args\": {\"noise_std\": 0.2, \"blur\": False}},\n",
    "    # only blur\n",
    "    {\"type\": \"blur\", \"args\": {\"noise_std\": 0.0, \"blur\": True}},\n",
    "    # both\n",
    "    {\"type\": \"noise+blur\", \"args\": {\"noise_std\": 0.2, \"blur\": True}}\n",
    "]\n",
    "\n",
    "# save robustness test metrics in a csv file\n",
    "with open(\"SQ3_MobileNet-V2.csv\", \"a\", newline='') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"model\", \"type\", \"acc\", \"precision\", \"recall\", \"f1\", \"auc\"])\n",
    "    if f.tell() == 0:\n",
    "        w.writeheader()\n",
    "    for test in test_types:\n",
    "        result = robustness_test(model, validation_loader, **test[\"args\"])\n",
    "        w.writerow({\"model\": \"MobileNet-V2\", \"type\": test[\"type\"], **result})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5085ddf1-f4be-45aa-8aeb-fbacdf72104c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model_path = r\"C:\\Users\\gjkku\\MobileNet-V2_best_model.pth\"\n",
    "if os.path.exists(best_model_path):\n",
    "    point = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(point)\n",
    "    model.eval()\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Best model file not found at {best_model_path}\")\n",
    "\n",
    "# lists to store all true labels and predictions\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in validation_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# computation of the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-Demented', 'Very Mild Demented'])\n",
    "\n",
    "# confusion matrix plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for MobileNet-V2')\n",
    "plt.savefig('MobileNet-V2_confusion_matrix.png')  \n",
    "plt.show()  \n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
